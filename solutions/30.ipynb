{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3dc246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def findSubstring(s: str, words: list[str]) -> list[int]:\n",
    "    if not s or not words:\n",
    "        return []\n",
    "        \n",
    "    word_len = len(words[0])\n",
    "    num_words = len(words)\n",
    "    sub_len = word_len * num_words\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(s) - sub_len + 1):\n",
    "        seen_count = Counter()\n",
    "        is_match = True\n",
    "        \n",
    "        for j in range(num_words):\n",
    "            word_start = i + j * word_len\n",
    "            word = s[word_start:word_start + word_len]\n",
    "            \n",
    "            if word not in word_count:\n",
    "                is_match = False\n",
    "                break\n",
    "                \n",
    "            seen_count[word] += 1\n",
    "            \n",
    "            if seen_count[word] > word_count[word]:\n",
    "                is_match = False\n",
    "                break\n",
    "        \n",
    "        if is_match:\n",
    "            result.append(i)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f256ee4",
   "metadata": {},
   "source": [
    "The LeetCode problem 30, \"Substring with Concatenation of All Words,\" is a highly challenging string search problem that involves finding all starting indices in a main string $S$ (the `haystack`) where a substring can be formed by concatenating all words from a given list of `words` (the `needle` set) **exactly once and without any intervening characters**. All words in the `words` list have the same length.\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem Constraints and Complexity**\n",
    "\n",
    "The difficulty of this problem arises from three constraints:\n",
    "1.  **Fixed Length:** The target substring must have a fixed, predetermined length, equal to $L \\times M$, where $L$ is the length of each word and $M$ is the number of words.\n",
    "2.  **Order Irrelevant:** The specific order of the words in the original `words` list does not matter; any permutation that forms a contiguous substring in $S$ is a valid match.\n",
    "3.  **Duplicates Must Be Handled:** If the `words` list contains duplicate words (e.g., `[\"foo\", \"bar\", \"foo\"]`), the matching substring must contain the word \"foo\" exactly twice and \"bar\" exactly once.\n",
    "\n",
    "These constraints make simple sliding window or KMP algorithms insufficient, necessitating a more sophisticated approach involving hashing and frequency counting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Preprocessing: The Master Frequency Map**\n",
    "\n",
    "The first crucial step is to preprocess the input `words` list to create a **master frequency map** (a hash map). This map stores the required count for every unique word in the input list. For example, if `words = [\"a\", \"b\", \"a\"]`, the master map would be `{\"a\": 2, \"b\": 1}`. This map provides the target fingerprint that every potential matching substring must satisfy. \n",
    "\n",
    "---\n",
    "\n",
    "### **The Sliding Window Strategy**\n",
    "\n",
    "The core of the solution is a fixed-size **sliding window** that moves through the main string $S$. The size of the window is fixed at $L_{\\text{total}} = \\text{length of word} \\times \\text{number of words}$. The brute-force approach would be to slide this window one character at a time, check the $L \\times M$ length substring, and verify its word frequencies against the master map. However, this check itself is slow.\n",
    "\n",
    "The optimal strategy involves a clever modification: the search is performed in $L$ (word length) distinct passes.\n",
    "\n",
    "---\n",
    "\n",
    "### **The $L$ Separate Passes (The $L$ Start Points)**\n",
    "\n",
    "Instead of checking all $N$ starting positions, we only need to check starting positions $i$ such that $0 \\le i < L$.\n",
    "* Pass 1: Start at index $0, L, 2L, 3L, \\dots$\n",
    "* Pass 2: Start at index $1, 1+L, 1+2L, 1+3L, \\dots$\n",
    "* ...\n",
    "* Pass $L$: Start at index $L-1, 2L-1, 3L-1, \\dots$\n",
    "\n",
    "This ensures that every possible valid starting position is covered exactly once. For each of these $L$ starting offsets, we run an internal sliding window specifically for that pass, where the window size is $L \\times M$ and the window only shifts by $L$ characters at a time. This effectively decouples the search into $L$ simpler, parallel problems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Internal Window Management and Verification**\n",
    "\n",
    "Within each of the $L$ passes, we maintain a **current frequency map** for the words currently inside the $L \\times M$ window. The window moves word by word (i.e., it slides by $L$ characters).\n",
    "\n",
    "1.  **Adding a Word:** A new word enters the window. We update the `current frequency map`.\n",
    "2.  **Checking for Validity:**\n",
    "    * If the new word is not in the `master frequency map`, the current window is invalid. We reset the `current frequency map`, move the start of the window past the current mismatch, and continue.\n",
    "    * If the frequency of the word in the `current map` exceeds its required frequency in the `master map`, the window has too many of that word. We shrink the window from the left (removing words) until the count is correct.\n",
    "3.  **Found Match:** If the number of words currently in the window equals $M$ (the total number of words), we have found a valid concatenation. The starting index of the window is added to the result list. We then immediately slide the window forward by one word (remove the leftmost word, add the next word) to continue the search for an overlapping match.\n",
    "\n",
    "---\n",
    "\n",
    "### **Complexity Analysis**\n",
    "\n",
    "* **Time Complexity:** The total number of words we examine is $N$ (length of $S$). Since we have $L$ passes, and in each pass, we perform $O(N/L)$ word operations, the total time spent across all passes is $L \\times O(N/L) = O(N)$. The time taken to hash and compare each word is $O(L)$, so the overall time complexity is $O(N \\cdot L)$, which is very efficient.\n",
    "* **Space Complexity:** We use space for the `master frequency map` ($O(M)$ or $O(\\text{unique words})$), the `current frequency map` ($O(M)$), and the result list. Thus, the auxiliary space complexity is $O(M \\cdot L)$, primarily dominated by storing the word maps and the result list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb666e6f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
